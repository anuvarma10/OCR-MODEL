import cv2
import time
import numpy as np
import onnx
import onnxruntime as ort
import torch
import torchvision.transforms as transforms
from sklearn.metrics import accuracy_score
from matplotlib import pyplot as plt

# Load your OCR model (assuming it's a PyTorch model here)
def load_model(model_path, device='cuda'):
    model = torch.load(model_path)
    model.to(device)
    model.eval()
    return model

# Convert PyTorch model to ONNX format
def convert_to_onnx(model, input_sample, onnx_path):
    torch.onnx.export(
        model,
        input_sample,
        onnx_path,
        export_params=True,
        opset_version=11,
        input_names=['input'],
        output_names=['output']
    )
    print(f"Model converted to ONNX format at {onnx_path}")

# Load ONNX model for CPU inference
def load_onnx_model(onnx_path):
    session = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])
    return session

# Perform inference on a single frame using ONNX model
def infer_onnx_model(session, frame):
    input_name = session.get_inputs()[0].name
    output_name = session.get_outputs()[0].name
    input_data = preprocess_frame(frame)
    result = session.run([output_name], {input_name: input_data})[0]
    return result

# Preprocess frame for model input
def preprocess_frame(frame):
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    frame = transform(frame).unsqueeze(0).numpy()
    return frame

# Evaluate both GPU and CPU models
def evaluate_models(video_path, model_path):
    # Load video file
    cap = cv2.VideoCapture(video_path)

    # Load GPU-based model
    gpu_model = load_model(model_path, device='cuda')

    # Convert GPU model to ONNX for CPU usage
    dummy_input = torch.randn(1, 3, 128, 128).to('cuda')  # Example input
    convert_to_onnx(gpu_model, dummy_input, 'ocr_model.onnx')

    # Load the converted CPU-based model
    cpu_model_session = load_onnx_model('ocr_model.onnx')

    # Initialize variables for performance evaluation
    frame_count = 0
    gpu_times, cpu_times = [], []

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        frame_count += 1

        # GPU Inference
        start_time = time.time()
        gpu_result = gpu_model(preprocess_frame(frame).to('cuda'))
        gpu_times.append(time.time() - start_time)

        # CPU Inference
        start_time = time.time()
        cpu_result = infer_onnx_model(cpu_model_session, frame)
        cpu_times.append(time.time() - start_time)

    cap.release()

    # Compute FPS for GPU and CPU
    gpu_fps = frame_count / sum(gpu_times)
    cpu_fps = frame_count / sum(cpu_times)

    # Print and compare results
    print(f"GPU FPS: {gpu_fps}, CPU FPS: {cpu_fps}")
    plot_performance(gpu_times, cpu_times)

def plot_performance(gpu_times, cpu_times):
    plt.figure(figsize=(10, 5))
    plt.plot(gpu_times, label='GPU Times')
    plt.plot(cpu_times, label='CPU Times')
    plt.xlabel('Frame')
    plt.ylabel('Time (s)')
    plt.legend()
    plt.title('GPU vs CPU Inference Times')
    plt.show()

if _name_ == "_main_":
    video_path = "path_to_video_file.mp4"
    model_path = "path_to_gpu_model.pt"  # Replace with the path to your GPU-based model
    evaluate_models(video_path, model_path)
